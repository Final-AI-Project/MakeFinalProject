# ------ ëª¨ë“ˆ ì„í¬íŠ¸
from multiprocessing import process
from fastapi import FastAPI
import dotenv
import os

# ------ FastAPI ì•±

app = FastAPI()

# ------ CORS
origins = [
    "http://localhost:8000",
    "http://127.0.0.1:8000",
    "http://localhost:3000",
    "http://127.0.0.1:3000",
    "http://localhost:19006",
    "http://127.0.0.1:19006",
    "http://localhost:5173",
    "http://127.0.0.1:5173",
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,        # í•„ìš” ì‹œ ["*"] ë¡œ ì „ì²´ í—ˆìš© ê°€ëŠ¥
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ----------------- ëª¨ë¸ ê²½ë¡œ
LLM_MODEL_PATH = process.env.LLM_MODEL_PATH
SPECIES_MODEL_PATH = process.env.SPECIES_MODEL_PATH
HEALTH_MODEL_PATH = process.env.HEALTH_MODEL_PATH
DISEASE_MODEL_PATH = process.env.DISEASE_MODEL_PATH

# -------------------- ë””ë°”ì´ìŠ¤/ë°ì´í„°íƒ€ì… ê²°ì • --------------------
USE_CUDA = torch.cuda.is_available()
USE_MPS = getattr(torch.backends, "mps", None) and torch.backends.mps.is_available()

if USE_CUDA:
    device = "cuda"
    dtype = torch.float16
elif USE_MPS:
    device = "mps"
    dtype = torch.float16
else:
    device = "cpu"
    dtype = torch.float32  # âœ… CPUëŠ” ë°˜ë“œì‹œ fp32 (half ë¯¸ì§€ì› ì—°ì‚° ìˆìŒ)

print(f"ğŸ”§ Device: {device}, dtype: {dtype}")

# -------------------- ëª¨ë¸ ë¡œë”© --------------------
print("ğŸ”§ Loading ControlNet...")
health_model = yolov8n-cls.from_pretrained(
    HEALTH_MODEL_PATH,
    torch_dtype=dtype,
)

print("ğŸ”§ Loading Base SD Pipeline...")
pipe = StableDiffusionControlNetPipeline.from_pretrained(
    BASE_MODEL_PATH,
    controlnet=controlnet,
    torch_dtype=dtype,
)

# LoRA ì ìš©
print("ğŸ”§ Applying LoRA...")
pipe.load_lora_weights(
    os.path.dirname(LORA_PATH),
    weight_name=os.path.basename(LORA_PATH)
)

# ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
pipe.to(device)

# (ì•ˆì •ì„±) í…ìŠ¤íŠ¸ ì¸ì½”ë”ëŠ” fp32 ê¶Œì¥
try:
    pipe.text_encoder.to(dtype=torch.float32, device=device)
except Exception:
    # ì¼ë¶€ ë²„ì „ì—ì„œ .to(dtype=...) ë¯¸ì§€ì›ì¼ ìˆ˜ ìˆìŒ â†’ ë¬´ì‹œ
    pass

# ë©”ëª¨ë¦¬ ìµœì í™”
try:
    if device == "cuda":
        pipe.enable_xformers_memory_efficient_attention()
    else:
        pipe.enable_attention_slicing()
except Exception:
    pipe.enable_attention_slicing()
    
    
    # -------------------------- í’ˆì¢… ë¶„ë¥˜ê¸° ë¡œì§ ë° API
    @app.post("/species")
    async def species(
        image: UploadFile = File(None),
        ): 
        upload = image
        result = health_model(upload)
        return json.dumps(result)
    
    
    # -------------------------- ì ìƒíƒœ ë¶„ë¥˜ê¸° ë¡œì§ ë° API
    @app.post("/health")
    
    # -------------------------- ì§ˆë³‘ ë¶„ë¥˜ê¸° ë¡œì§ ë° API
    @app.post("/disease")
    
    # -------------------------- LLM ì²˜ë¦¬ ë¡œì§ ë° API
    @app.post("/llm")
    