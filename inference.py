# inference.py (ì…ë ¥ì±„ë„ ìë™ê°ì§€ + 5ì±„ë„ ì „ì²˜ë¦¬ + ì•ˆì „í•œ ê²½ë¡œ)
import argparse, sys
from pathlib import Path

import torch
import torch.nn.functional as F
from timm import create_model
from torchvision import transforms as T
from PIL import Image
import torch.nn as nn
import torch.nn.functional as Fnn

# ìœ ì—° ì„í¬íŠ¸ (íŒ¨í‚¤ì§€/ì§ì ‘ ì‹¤í–‰ ëª¨ë‘ ì§€ì›)
try:
    from pland_disease.NLG import generate_response
    from pland_disease.disease import DISEASE_INFO as disease_info
except ModuleNotFoundError:
    from NLG import generate_response
    from disease import DISEASE_INFO as disease_info

# ---------- 5ì±„ë„ ì „ì²˜ë¦¬ ----------
class AddTextureChannels(nn.Module):
    """RGB [3,H,W] -> [5,H,W]: RGB + SobelMag + Laplacian (0~1 ì •ê·œí™”)"""
    def __init__(self):
        super().__init__()
        kx  = torch.tensor([[-1,0,1],[-2,0,2],[-1,0,1]], dtype=torch.float32).view(1,1,3,3)
        ky  = torch.tensor([[-1,-2,-1],[0,0,0],[1,2,1]], dtype=torch.float32).view(1,1,3,3)
        lap = torch.tensor([[0,-1,0],[-1,4,-1],[0,-1,0]], dtype=torch.float32).view(1,1,3,3)
        self.register_buffer("kx", kx); self.register_buffer("ky", ky); self.register_buffer("lap", lap)

    def forward(self, x):  # x: [C,H,W] in [0,1]
        if x.dim() == 3:
            x = x.unsqueeze(0)  # [1,3,H,W]
        r, g, b = x[:,0:1], x[:,1:2], x[:,2:3]
        gray = 0.2989*r + 0.5870*g + 0.1140*b
        gx = Fnn.conv2d(gray, self.kx, padding=1)
        gy = Fnn.conv2d(gray, self.ky, padding=1)
        sobel = torch.sqrt(gx*gx + gy*gy + 1e-8)
        lap   = Fnn.conv2d(gray, self.lap, padding=1).abs()

        def norm01(t):
            mn = t.amin(dim=(2,3), keepdim=True)
            mx = t.amax(dim=(2,3), keepdim=True)
            return (t - mn) / (mx - mn + 1e-8)

        sobel = norm01(sobel)
        lap   = norm01(lap)
        out = torch.cat([x, sobel, lap], dim=1)  # [1,5,H,W]
        return out.squeeze(0)

# ---------- ì¸ì ----------
ap = argparse.ArgumentParser()
ap.add_argument("--weights", type=str, default=None, help="path to best.pt (ê¸°ë³¸: íŒ¨í‚¤ì§€ models/best.pt)")
ap.add_argument("--input", type=str, default="samples", help="ì´ë¯¸ì§€ íŒŒì¼ ë˜ëŠ” í´ë”")
ap.add_argument("--topk", type=int, default=3)
ap.add_argument("--nickname", type=str, default="ìš°ë¦¬ ì‹ë¬¼")
ap.add_argument("--species", type=str, default="ìŠ¤íˆ¬í‚¤")
args = ap.parse_args()

# ---------- ê²½ë¡œ ----------
PKG_DIR = Path(__file__).resolve().parent
DEFAULT_WEIGHTS = PKG_DIR / "models" / "best.pt"
WEIGHTS = Path(args.weights) if args.weights else DEFAULT_WEIGHTS

if not WEIGHTS.exists():
    print(f"âŒ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {WEIGHTS.resolve()}")
    print("ğŸ’¡ ì˜ˆ: python -m pland_disease.inference --weights models\\best.pt --input samples")
    sys.exit(1)

# ---------- ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ----------
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
ckpt = torch.load(str(WEIGHTS), map_location=device)
classes = ckpt["classes"]
cfg = ckpt.get("cfg", {})
MODEL_NAME = cfg.get("model", "efficientnet_b0")
IMG_SIZE   = cfg.get("img_size", 400)

# ì…ë ¥ ì±„ë„ ìë™ ê°ì§€
state_dict = ckpt["model"]
conv_key = "conv_stem.weight"
if conv_key in state_dict:
    in_chans = int(state_dict[conv_key].shape[1])
else:
    in_chans = int(cfg.get("in_chans", 3))

# ---------- ëª¨ë¸ ----------
model = create_model(MODEL_NAME, pretrained=False, num_classes=len(classes), in_chans=in_chans)
model.load_state_dict(state_dict, strict=False)
model.to(device).eval()

# ---------- ì „ì²˜ë¦¬ ----------
MEAN_RGB, STD_RGB = (0.485,0.456,0.406), (0.229,0.224,0.225)
MEAN_5 = list(MEAN_RGB) + [0.5, 0.5]
STD_5  = list(STD_RGB)  + [0.5, 0.5]

if in_chans == 5:
    preprocess = T.Compose([
        T.Resize(IMG_SIZE),
        T.CenterCrop(IMG_SIZE),
        T.ToTensor(),
        AddTextureChannels(),
        T.Normalize(MEAN_5, STD_5),
    ])
else:
    preprocess = T.Compose([
        T.Resize(IMG_SIZE),
        T.CenterCrop(IMG_SIZE),
        T.ToTensor(),
        T.Normalize(MEAN_RGB, STD_RGB),
    ])

# ---------- ìœ í‹¸ ----------
def list_images(path: Path):
    exts = {".jpg", ".jpeg", ".png", ".bmp", ".webp"}
    p = Path(path)
    if p.is_file() and p.suffix.lower() in exts:
        return [p]
    if p.is_dir():
        return sorted([x for x in p.rglob("*") if x.suffix.lower() in exts])
    return []

# ---------- ì¶”ë¡  ----------
@torch.no_grad()
def predict(img_path: Path, plant_nickname: str, plant_species: str, topk: int = 3):
    img = Image.open(img_path).convert("RGB")
    x = preprocess(img).unsqueeze(0).to(device)

    logits = model(x)
    probs = torch.softmax(logits, dim=1)[0]
    top_p, top_c = probs.topk(topk)
    preds = [(classes[int(i)], float(p)) for p, i in zip(top_p.tolist(), top_c.tolist())]

    # NLG
    response = generate_response(plant_nickname, plant_species, preds)

    # ë¡œê·¸
    print(f"\nğŸ“· {img_path.name}")
    for rank, (label, _) in enumerate(preds, start=1):
        print(f" - {rank}ìˆœìœ„: {label}")
    print("ğŸ’¬", response)

    return preds, response

# ---------- main ----------
def main():
    src = Path(args.input)
    imgs = list_images(src)
    if not imgs:
        print("âŒ ì…ë ¥ ê²½ë¡œì—ì„œ ì´ë¯¸ì§€ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤:", src.resolve())
        sys.exit(1)
    for p in imgs:
        predict(p, args.nickname, args.species, args.topk)

if __name__ == "__main__":
    main()
